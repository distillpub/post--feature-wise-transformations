<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <link href="assets/style.css" type="text/css" rel="stylesheet">
    <script src="https://distill.pub/template.v1.js"></script>
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
          integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
          crossorigin="anonymous">
    <script src="https://d3js.org/d3.v4.min.js"></script>
    <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>

    <script type="text/front-matter">
      title: "Understanding FiLM"
      description: "Description of the post"
      authors:
      - Vincent Dumoulin: https://vdumoulin.github.io
      - Ethan Perez: http://ethanperez.net/
      - Florian Strub: https://fstrub95.github.io/
      - Harm de Vries: http://www-etud.iro.umontreal.ca/~devries/
      - Nathan Schucher: https://nathanschucher.com/
      - Aaron Courville: https://aaroncourville.wordpress.com/
      - Yoshua Bengio: http://www.iro.umontreal.ca/~bengioy/yoshua_en/
      affiliations:
      - MILA: https://mila.quebec/en/
      - Rice University: http://www.rice.edu/
      - Inria SequeL: https://team.inria.fr/sequel/
      - MILA: https://mila.quebec/en/
      - Element AI: https://element.ai/
      - MILA: https://mila.quebec/en/
      - MILA: https://mila.quebec/en/
    </script>
  </head>
  <body>
    <dt-article>
      <h1>Understanding feature-wise linear modulation (FiLM) layers</h1>
      <h2><span class="todo"> TODO: Write a description of the article.</span></h2>

      <dt-byline></dt-byline>

      <p>
        Much like attentional interfaces, which have been the subject of a
        previous <a href="https://distill.pub/2016/augmented-rnns/">Distill
        article</a>, feature-wise affine transformations have emerged as a
        powerful mechanism which allows to modulate the computation carried out
        by a neural network.
      </p>
      <p>
        To center the discussion around a common language, we propose to use
        the nomenclature of <dt-cite key="perez2017film"></dt-cite>, which
        calls these transformations <em>FiLM</em>, for
        <strong>F</strong>eature-w<strong>i</strong>se <strong>L</strong>inear
        <strong>M</strong>odulation.
        <dt-fn>
          Strictly speaking, <em>linear</em> is a misnomer, as we allow biasing,
          but we hope the more rigorous-minded reader will forgive us for the
          sake of a better-sounding acronym.
        </dt-fn>
      </p>
      <p>
        We say that a neural network is modulated using FiLM, or
        <em>FiLM-ed</em>, through the insertion of <em>FiLM layers</em> in its
        architecture. These are parametrized by some form of conditioning
        information, and the mapping from conditioning information to FiLM
        parameters is called the <em>FiLM generator</em>. For simplicity, you
        can assume that the FiLM generator outputs the concatenation of all
        FiLM parameters for the network architecture.
      </p>
      <figure class="l-body-outset figure" id="film-architecture-diagram">
        <figcaption>
          In this example, a network is FiLM-ed by inserting two FiLM layers in
          its architecture. The FiLM parameters are computed using the FiLM
          generator, which processes the conditioning information.
        </figcaption>
      </figure>
      <p>
        As the name implies, a FiLM layer applies a feature-wise affine
        transformation to its input. The parameters of that transformation are
        also provided as input. By <em>feature-wise</em>, we mean that scaling
        and shifting are applied elementwise, or in the case of convolutional
        networks, feature map-wise. In other words,

        <dt-math block>
            \textrm{FiLM}(\mathbf{x}) = \gamma(\mathbf{c}) \odot \mathbf{x}
                                                           + \beta(\mathbf{c}),
        </dt-math>

        where <dt-math>\mathbf{x}</dt-math> is the FiLM layer's input and
        <dt-math>\mathbf{c}</dt-math> is the conditioning information. You can
        interact with the following fully-connected and convolutional FiLM
        layers to get an intuition of the sort of modulation they allow:
      </p>
      <figure class="l-body-outset figure" id="film-diagram">
        <figcaption>
          A FiLM layers applied on a fully-connected input (left) and on a
          convolutional input (right). In both cases, the scaling and shifting
          vectors <dt-math>\gamma</dt-math> and <dt-math>\beta</dt-math> are
          passed as input and are computed by the FiLM generator. Try
          interacting with the <dt-math>\gamma</dt-math> and
          <dt-math>\beta</dt-math> values by hovering over their corresponding
          diagram elements and observe the resulting change in the output.
        </figcaption>
      </figure>
      <p>
        FiLM shares with HyperNetworks <dt-cite key="ha2016hypernetworks"></dt-cite>
        the architectural feature that a secondary neural network computes some
        of the parameter values in the primary network. From that perspective,
        the FiLM generator can be viewed as a specialized instance of a
        HyperNetwork which predicts the FiLM parameters of the FiLM-ed network.
      </p>
      <p>
        Feature-wise affine transformations are closely related to gating: the
        latter can be viewed as a bias-less version of the former with
        sigmoidal scaling. Whereas gating is used to inhibit the expression of
        targeted features, FiLM can in theory be used to inhibit or enhance the
        expression of targeted features, flip their sign, or selectively
        threshold them (when combined with a ReLU nonlinearity). The impact of
        these additional capabilities is still an open area of investigation.
      </p>
      <p>
        FiLM is also closely connected to gating, attention, and
        mixture-of-experts models through bilinear transformations
        <dt-cite key="tenenbaum2000separating"></dt-cite>. Because this
        connection deserves more than a simple paragraph, we decided to devote a
        <a href="#bilinear-connection">separate section</a> of the article to it.
      </p>
      <p>
        Finally, although attempts to connect deep learning results to the way
        the brain works should always be treated carefully, it is interesting to
        note that the idea of feature-wise modulation finds a modest amount of
        support in the neuroscience literature.
      </p>
      <p>
        In an attempt to explore biologically plausible neuron models while
        drawing a parallel with multi-layer perceptrons,
        <dt-cite key="mel1992clusteron"></dt-cite> introduces the
        <em>Clusteron</em>, a neuron whose activations can be amplified by
        surrounding neurons. This behavior enables to extract non-linearity and
        high-order statistics from data.
      </p>
      <p>
        Work in <dt-cite key="boutonnet2015words"></dt-cite> provides new
        evidence that external language cues alter how visual information is
        processed in the brain. More precisely, it is observed that P1 signals,
        which are related to low-level visual features, are modulated while
        hearing specific words.
      </p>

      <h2>Table of contents</h2>
      <p>
        Because of their very general nature and wide applicability,
        feature-wise affine transformations are sometimes treated as accessory
        to the proposed models to which they are applied. We believe there is
        value in connecting those methods, which is why part of this article is
        devoted to a literature review. In the spirit of the
        <a href="https://distill.pub/2016/augmented-rnns/"> Distill article</a>
        on attention, we outlined several directions through which feature-wise
        affine transformations express themselves in the literature:
      </p>
      <ul>
        <li><p><a href="#conditioning">Conditioning on external information</a></p>
          <ul>
            <li><a href="#conditioning-film">FiLM</a></li>
            <li><a href="#conditioning-gating">Gating</a></li>
            <li><a href="#conditioning-biasing">Conditional biasing</a></li>
            <li><a href="#conditioning-concatenation">Concatenation</a></li>
          </ul>
        </li>
        <li><p><a href="#self-conditioning">Self-conditioning</a></p>
          <ul>
            <li><a href="#self-conditioning-film">FiLM</a></li>
            <li><a href="#self-conditioning-gating">Gating</a></li>
            <li><a href="#self-conditioning-featurewise-attention">Feature-wise attention</a></li>
          </ul>
        </li>
      </ul>
      <p>
        We also attempt to characterize and explain the behavior of FiLM-ed
        networks through <a href="#understanding-film">analyses on toy and
        real-world tasks</a>.
      </p>
      <p>
        Finally, as noted above, we reserved a section of this article for the
        discussion of the <a href="#bilinear-connection">connection between
        bilinear transformations, FiLM, gating, attention, and
        mixture-of-experts models</a>.
      </p>

      <hr />

      <h1>Feature-wise affine transformations in the literature</h1>
      <p>
        Throughout this section, we will present various models which take
        advantage of feature-wise affine transformations to show how widely
        applied this computational mechanism is in practice. We try
        to provide a comprehensive overview, but we do not claim that it is
        complete.
      </p>
      <p>
        In the interest of not being confusing to readers already familiar with
        these models, we chose to stick to the nomenclature used in the original
        papers, but we do draw connections to the FiLM nomenclature where
        appropriate. Readers interested in those connections are encouraged
        to take a closer look at the model diagrams, in which elements relevant
        to the FiLM nomenclature have been color-coded in yellow and can be
        hovered over to reveal more information.
      </p>

      <h2 id="conditioning">Conditioning on external information</h2>
      <p>
        Feature-wise affine transformations can be used as a conditioning
        mechanism to incorporate external information such as context
        representation, task description, class information, or user
        information. While FiLM represents the most general instantiation of a
        feature-wise affine transformation, gating, conditional biasing, and
        concatenation &mdash; all of which can be seen as a special case
        of FiLM &mdash; are also widely used in practice.
      </p>

      <h3 id="conditioning-film">FiLM</h3>
      <p>
        We can use FiLM layers to condition a visual processing pipeline to
        answer textual questions about an image.
      </p>
      <p>
        In <dt-cite key="perez2017learning"></dt-cite>, and in its expanded
        version <dt-cite key="perez2017film"></dt-cite>, a visual reasoning
        model is trained on the CLEVR dataset <dt-cite key="johnson2017clevr"></dt-cite>
        to answer synthetic questions about a synthetic input image.
      </p>
      <figure class="l-body-outset figure" id="clevr-diagram">
        <figcaption>
          Visual reasoning model trained on CLEVR. The residual blocks of the
          visual pipeline are FiLM-ed, and the linguistic pipeline acts as the
          FiLM generator.
        </figcaption>
      </figure>
      <p>
        The model features a FiLM generator in the form of a linguistic pipeline
        extracting a question representation which is then linearly mapped to
        the FiLM parameter values. The visual pipeline is conditioned by
        inserting a FiLM layer in the residual pathway of each residual block of
        the convolutional neural network. The model is trained end-to-end on
        image-question-answer tuples.
      </p>
      <p>
        We can also leverage feature-wise affine transformations to condition a
        pre-trained network. In in <dt-cite key="vries2017modulating"></dt-cite>,
        the visual pipeline of a visual question-answering (VQA) model is
        modulated by the model's linguistic pipeline through conditional batch
        normalization (CBN). The model is trained on the "oracle" sub-task of
        the GuessWhat?! task <dt-cite key="vries2017guesswhat"></dt-cite>,
        which features natural language questions on images which are answered
        by yes, no, or N/A.
      </p>
      <figure class="l-body-outset figure" id="guesswhat-diagram">
        <figcaption>
          Visual question-answering (VQA) model trained on the oracle task of
          the GuessWhat?! dataset. The feature-wise affine transformation inside
          the batch normalization layers of the pre-trained ResNet visual
          pipeline are modulated by the linguistic pipeline.
        </figcaption>
      </figure>
      <p>
        The visual pipeline consists of a pre-trained residual network and is
        kept fixed throughout training. The linguistic pipeline interacts with
        the visual pipeline by predicting perturbations to the residual
        network's batch normalization parameters.
      </p>
      <p>
        CBN can be viewed as an instance of FiLM where the post-normalization
        feature-wise affine transformation is replaced with a FiLM layer.
      </p>
      <p>
        We can also use FiLM layers to condition a style transfer network on a
        chosen style image.
      </p>
      <p>
        In <dt-cite key="dumoulin2017learned"></dt-cite>, fast feedforward style
        transfer networks are extended to accommodate multiple styles by
        introducing conditional instance normalization (CIN) layers. Each style
        modeled by the network is associated with its own set of instance
        normalization parameters, and conditioning is achieved by assigning
        instance normalization parameters their corresponding value for the
        desired style.
      </p>
      <figure class="l-body-outset figure" id="alrfas-diagram">
        <figcaption>
          Fast, arbitrary artistic style transfer model. The style prediction
          network predicts the value of the instance normalization layers'
          feature-wise affine transformation parameters from the style image.
        </figcaption>
      </figure>
      <p>
        This intuition is pushed further by <dt-cite key="ghiasi2017exploring"></dt-cite>
        with the introduction of a <em>style prediction network</em> which is
        trained jointly with the style transfer network to predict the instance
        normalization parameters directly from a given style image.
      </p>
      <p>
        Like CBN, CIN can be seen as an instance of FiLM where the
        post-normalization feature-wise affine transformation is replaced with a
        FiLM layer. The main differentiator between <dt-cite key="dumoulin2017learned"></dt-cite>
        and <dt-cite key="ghiasi2017exploring"></dt-cite> is the FiLM generator
        implementation: in the former the generator is a simple embedding
        lookup, whereas in the latter the generator is a convolutional neural
        network.
      </p>
      <p>
        So far, the distinction between the FiLM generator and the FiLM-ed
        network has been rather clear, but it is not strictly necessary. As an
        example, in <dt-cite key="huang2017arbitrary"></dt-cite> proposes an
        alternative for fast and arbitrary style transfer in the form of an
        adaptive instance normalization (AdaIN) layer.
      </p>
      <figure class="l-body-outset figure" id="adain-diagram">
        <figcaption>
          Another approach to fast, arbitrary artistic style transfer models.
          When applying instance normalization to the output of the encoder
          given the content image, the instance normalization layer's
          feature-wise affine transformation parameters are computed as the
          style image's instance normalization statistics in the same layer.
        </figcaption>
      </figure>
      <p>
        The model architecture consists of an encoder-decoder pair. The encoder
        is the same classifier that is used to compute the content and style
        losses, and is kept fixed throughout training. A content/style image
        pair is first encoded, and the encoded stack of <em>content</em> feature
        maps is instance-normalized. Instance normalization statistics are then
        computed for the encoded stask of <em>style</em> feature maps and used
        to perform a feature-wise affine transformation on the normalized stack
        of content feature maps. The result is then passed through the decoder
        to obtain the stylized image, and the triplet of content/style/stylized
        images is used to compute the style transfer loss. The decoder is
        trained on many content/style image pairs to minimize this loss.
      </p>
      <p>
        Interestingly, the encoder can be seen as both <em>both</em> a part of
        the FiLM-ed network, <em>and</em> the FiLM generator <em>itself</em>.
        Therefore, AdaIN can be recognized as a FiLM layer replacing the
        instance normalization parameters, with the FiLM-generating network
        taking the form of the same trained classifier being used to extract
        instance normalization statistics on the style image.
      </p>
      <p>
        Feature-wise affine transformations are also used in the reinforcement
        learning setting. In <dt-cite key="kirkpatrick2017overcoming"></dt-cite>,
        the authors briefly touch the idea of applying conditional feature-wise
        affine transformations to a reinforcement learning problem. They train
        an agent that learns to play 10 Atari games using a single Double DQN
        network with task-specific gains and biases, which can be viewed as
        inserting FiLM layers throughout its hierarchy.
      </p>

      <h3 id="conditioning-gating">Gating</h3>
      <p>
        Work in <dt-cite key="chaplot2017gated"></dt-cite> uses feature-wise
        sigmoidal gating to fuse linguisting and visual inputs in the context
        of a reinforcement learning (RL) agent trained to follow simple natural
        language instructions in a 3D environment (VizDoom).
      </p>
      <figure class="l-body-outset figure" id="gated-attention-diagram">
        <figcaption>
          Gated-attention fusion unit. The instruction representation is mapped
          to a feature vector with as many elements as the number of image
          representation feature maps through a sigmoidal layer. This feature
          vector is then multiplied element-wise &mdash; using broadcasting
          &mdash; with the image representation.
        </figcaption>
      </figure>
      <p>
        Feature-wise sigmoidal gating can be seen as inserting a FiLM layer
        with scaling constrained to be between 0 and 1 via a sigmoidal layer
        and shifting set to zero.
      </p>
      <p>
        Work in <dt-cite key="dhingra2017gated"></dt-cite> introduces the
        Gated-Attention Reader, which conditions a document reading network
        with an associated query via multiple steps of feature-wise
        multiplicative gating. Each token in the document sequence is used to
        extract a token-specific query representation via soft attention over
        the query sequence, and the query representation is multiplied
        feature-wise with the token.
      </p>

      <h3 id="conditioning-biasing">Conditional biasing</h3>
      <p>
        PixelCNN <dt-cite key="oord2016conditional"></dt-cite> and WaveNet
        <dt-cite key="oord2016wavenet"></dt-cite>, two recent advances in
        autoregressive modeling of image and audio data, respectively, employ a
        similar conditional biasing scheme. The inner workings of their
        autoregressive formulation is beyond the scope of this article, but
        their conditioning scheme can be explained from the FiLM perspective. 
      </p>
      <p>
        In its simplest form, conditioning in PixelCNN models is achieved by
        adding feature-wise, class-conditional biases to the output of all
        convolutional layers. Like for DCGAN, this can be viewed as inserting
        FiLM layers after convolutional layers and expressing the
        FiLM-generating network as an embedding lookup on the class labels,
        with the scaling coefficients set to a constant value of 1. The authors
        also describe a location-dependent biasing scheme which cannot be
        expressed in terms of FiLM layers due to the absence of the
        feature-wise property.
      </p>
      <figure class="l-body-outset figure" id="pixelcnn-diagram">
        <figcaption>
          A PixelCNN layer is conditioned by biasing its sigmoidal and
          hyperbolic tangent layers as a function of a high-level image
          description.
        </figcaption>
      </figure>
      <p>
        WaveNet describes two ways in which conditional biasing allows external
        information to modulate the behavior of the model:
      </p>
      <ol>
        <li>
          <strong>Global conditioning</strong> applies the same conditional
          bias to the whole generated sequence and is used e.g. to condition on
          speaker identity.
        </li>
        <li>
          <strong>Local conditioning</strong> applies a conditional bias which
          varies across time steps of the generated sequence and is used e.g.
          to let linguistic features in a text-to-speech (TTS) model influence
          which sounds are produced.
        </li>
      </ol>
      <p>
        Like for PixelCNN, both approaches can be viewed as inserting FiLM
        layers after each convolutional layer. The main difference lies in the
        way in which the FiLM-generating network is defined: global conditioning
        expresses the FiLM-generating network as an embedding lookup which is
        broadcasted to the whole time series, whereas local conditioning
        expresses it as a mapping from an input sequence of conditioning
        information to an output sequence of FiLM parameters.
      </p>

      <h3 id="conditioning-concatenation">Concatenation</h3>
      <p>
        DCGAN <dt-cite key="radford2016unsupervised"></dt-cite>, a
        well-recognized class of network architectures for generative
        adversarial networks (GANs), lends itself to class conditioning by
        concatenating the class label broadcasted as a feature map to the input
        of convolutional and transposed convolutional layers in the generator
        and discriminator networks. An equivalent &mdash; and more efficient
        &mdash; way to express this operation is in the form of a feature-wise,
        class-conditional biasing of the output of the convolutional or
        transposed convolutional layer.
        <dt-fn>
          Provided we ignore the border effects due to zero-padding or use an
          alternative form of padding, like reflection-padding.
        </dt-fn>
        We can therefore view the class-conditional version of
        DCGAN as inserting FiLM layers after each convolutional and transposed
        convolutional layer. The FiLM-generating network comes in the form of
        an embedding lookup on the class labels, with the scaling coefficients
        set to a constant value of 1.
      </p>
      <figure class="l-body-outset figure" id="dcgan-diagram">
        <figcaption>
          Concatenation and conditional biasing are equivalent.
          <dt-fn>
            Provided we ignore the border effects due to zero-padding or use an
            alternative form of padding, like reflection-padding.
          </dt-fn>
          In this
          example, convolving any kernel over a conditioning feature
          broadcasted into a feature map results in a constant-valued output
          feature map, which is equivalent to adding a conditional bias
          broadcasted to the size of the output feature map.
        </figcaption>
      </figure>

      <h2 id="self-conditioning">Self-conditioning</h2>
      <p>
        So far the applications we have considered condition on <em>external</em>
        information, but it is also possible (and sometimes desireable) to
        condition on <em>internal</em> information &mdash; that is, to
        <em>self-condition</em> a network. Once again, in addition to FiLM,
        various special cases are used in practice.
      </p>

      <h3 id="self-conditioning-film">FiLM</h3>
      <p>
        In the speech recognition domain, <dt-cite key="kim2017dynamic"></dt-cite>
        modulates a deep bidirectional LSTM using a form of conditional
        normalization which the authors call <em>dynamic layer normalization</em>.
        In analogy to conditional instance normalization and conditional batch
        normalization, dynamic layer normalization can be seen as an instance of
        FiLM where the post-normalization feature-wise affine transformation is
        disabled and a FiLM layer is placed immediately after the instance
        normalization layer.
      </p>
      <figure class="l-body-outset figure" id="dln-diagram">
        <figcaption>
          The speech recognition model in <dt-cite key="kim2017dynamic"></dt-cite>
          achieves speaker adaptation by predicting the layer normalization
          scaling and shifting coefficients inside its LSTMs' forget, input,
          and output gates from an utterance summarization vector.
        </figcaption>
      </figure>
      <p>
        The key difference here is that the conditioning signal does not come
        from an external source, but is rather computed from utterance
        summarization feature vectors extracted in each layer of the network
        as a way of adapting the neural acoustic model.
      </p>

      <h3 id="self-conditioning-gating">Gating</h3>
      <p>
        In the visual domain, the ImageNet 2017 winning model
        <dt-cite key="hu2017squeeze"></dt-cite> employs a self-conditioning
        scheme in the form of feature-wise sigmoidal gating as a way to
        condition a layer's activations on its previous layer.
      </p>
      <figure class="l-body-outset figure" id="squeeze-diagram">
        <figcaption>
          The squeeze-and-excitation block implements a self-conditioned
          feature-wise sigmoidal gating operation.
        </figcaption>
      </figure>
      <p>
        In the natural language domain, <dt-cite key="dauphin2017language"></dt-cite>
        proposes to build a finite context using stacked convolutions to tackle
        language modeling and introduces gated linear units, which uses half of
        the features to apply feature-wise sigmoidal gating to the second half.
        This architectural feature is also adopted by
        <dt-cite key="gehring2017convolutional"></dt-cite>, which introduces a
        fast and parallelizable model for machine translation in the form of a
        purely convolutional architecture.
      </p>
      <figure class="l-body-outset figure" id="convseq2seq-diagram">
        <figcaption>
          Gated linear unit activation function <dt-cite key="dauphin2017language"></dt-cite>.
          Half of the features are used to apply feature-wise sigmoidal gating
          to the second half of the features.
        </figcaption>
      </figure>
      <p>
        From the FiLM point of view, feature-wise sigmoidal gating is
        equivalent to a FiLM layer with the scaling being passed through a
        sigmoidal non-linearity and the shifting set to <dt-math>0</dt-math>.
      </p>
      <p>
        The LSTM <dt-cite key="hochreiter1997long"></dt-cite> architecture,
        which is itself a building block in some of the models we referenced
        thus far, makes heavy use of feature-wise sigmoidal gating in its input,
        forget, and output gates.
      </p>
      <p>
        Finally, in addition to conditional biasing,
        PixelCNN <dt-cite key="oord2016conditional"></dt-cite> and
        WaveNet <dt-cite key="oord2016wavenet"></dt-cite> both employ a
        self-conditioned tanh-sigmoid gated nonlinearity.
      </p>

      <h3 id="self-conditioning-featurewise-attention">Feature-wise attention</h3>
      <p>
        Some work even combines attention and feature-wise affine
        transformations. Work in <dt-cite key="shen2017disan"></dt-cite>
        proposes a feature-wise self-attention mechanism for language
        understanding, showing strong results on datasets such as SNLI
        <dt-cite key="bowman2015large"></dt-cite>. A core idea behind the
        proposed model, named Directional Self-Attention Network (DiSAN), is
        that of multi-dimensional attention, i.e., combining tokens at a
        <em>feature</em> level rather than at a <em>feature vector</em> level,
        using the input itself to generate the feature-wise attention
        distributions over all tokens.
      </p>
      <figure class="l-body-outset figure" id="disan-diagram">
        <figcaption>
          WRITEME
        </figcaption>
      </figure>

      <hr />

      <h1 id="understanding-film">Understanding FiLM-ed networks</h1>

      <h2>FiLM parameters as task embeddings</h2>
      <div class="l-body-outset figure" id="clevr-tsne"></div>

      <hr />

      <h1 id="bilinear-connection">Connecting FiLM, gating, attention, and mixture-of-experts models</h1>
      <p>
        As previously hinted at, there exists a strong connection between FiLM,
        gating, attention, and mixture-of-experts models through bilinear
        transformations <dt-cite key="tenenbaum2000separating"></dt-cite>. In
        fact, all four approaches can be formalized using an extension of
        bilinear transformations which we propose to name <em>generalized
        bilinear transformations</em>.
      </p>
      <h2>Bilinear and generalized bilinear transformations</h2>
      <p>
        A bilinear transformation defines the relationship between two inputs
        <dt-math>\mathbf{x}</dt-math> and <dt-math>\mathbf{z}</dt-math> and the
        <dt-math>k^{th}</dt-math> output feature <dt-math>y_k</dt-math> as

        <dt-math block>
          y_k = \mathbf{x}^T W_k \mathbf{z}.
        </dt-math>

        Note that for each output feature <dt-math>y_k</dt-math> we have a
        separate matrix <dt-math>W_k</dt-math>, so the full set of weights forms
        a 3-tensor.
      </p>
      <figure class="l-body-outset figure" id="bilinear-diagram">
        <figcaption>
          A bilinear transformation mapping <dt-math>\mathbf{x}</dt-math> and
          <dt-math>\mathbf{z}</dt-math> &mdash; both 3-dimensional &mdash; to a
          3-dimensional output. Each output element <dt-math>y_k</dt-math> is
          computed as a vector-matrix-vector product using a separate matrix
          <dt-math>W_k</dt-math>.
        </figcaption>
      </figure>
      <p>
        In general, bilinear transformations are used to combine two inputs
        without attaching a specific meaning to the nature of those inputs. For
        the purpose of our discussion, we will consider
        <dt-math>\mathbf{x}</dt-math> to be the representation being FiLM-ed,
        gated, or attended to, and <dt-math>\mathbf{z}</dt-math> to be the
        context used to FiLM, gate, or attend to.
      <p>
        The generalized bilinear transformation we propose stretches the
        definition of a bilinear transformation in two ways:
        <ul>
          <li>
            It appends a <dt-math>1</dt-math>-valued feature to the inputs
            <dt-math>\mathbf{x}</dt-math> and <dt-math>\mathbf{z}</dt-math> to
            accommodate affine transformations &mdash; a common trick used to
            simplify the algebra of neural networks:
            <dt-math block>
              \begin{aligned}
                \tilde{\mathbf{x}} &= (x_0, \ldots, x_M, 1)^T, \\
                \tilde{\mathbf{z}} &= (z_0, \ldots, z_N, 1)^T.
              \end{aligned}
            </dt-math>
          </li>
          <li>
            It applies an activation function <dt-math>g_k(\cdot)</dt-math> to
            the result of the matrix-vector product:
            <dt-math block>
              y_k = \tilde{\mathbf{x}}^T g_k(W_k \tilde{\mathbf{z}}).
            </dt-math>
          </li>
        </ul>
      </p>
      <figure class="l-body-outset figure" id="generalized-bilinear-diagram">
        <figcaption>
          A generalized bilinear transformation mapping mapping the inputs
          <dt-math>\mathbf{x}</dt-math> and <dt-math>\mathbf{z}</dt-math>
          &mdash; both 3-dimensional &mdash; to a 3-dimensional output.
        </figcaption>
      </figure>
      <p>
        The choice of activation function <dt-math>g_k(\cdot)</dt-math> and the
        structure of the weight matrices <dt-math>W_k</dt-math> determines the
        nature of the transformation: FiLM, gating, or attention. Going
        backwards from the definition of each computation mechanism, we will now
        explain how they can be expressed in terms of generalized bilinear
        transformations. To simplify the discussion, we give the name
        <em>context vector</em> to the matrix-vector product:

        <dt-math block>
          \mathbf{v} = g_k(W_k \tilde{\mathbf{z}})
        </dt-math>
      </p>

      <h2>Connecting the dots</h2>

      <h3>FiLM as a generalized bilinear transformation</h3>
      <p>
        FiLM is represented by an equation of the form

        <dt-math block>
          y_k = \gamma_k x_k + \beta_k.
        </dt-math>

        This means that the nonlinearity should be the identity function
        <dt-math>g_k(\mathbf{u}) = \mathbf{z}</dt-math>, and the context vector
        should have the form

        <dt-math block>
          \mathbf{v} = (0, \ldots, 0, \gamma_k, 0, \ldots, 0, \beta_k)^T,
        </dt-math>

        where <dt-math>\gamma_k</dt-math> is the <dt-math>k^{th}</dt-math>
        element of the context vector. This means in turn that the weight matrix
        <dt-math>W_k</dt-math> should be low-rank and have the form

        <dt-math block>
          W_k =
          \begin{pmatrix}
             0 & 0 \\
             \vdots & \vdots \\
             0 & 0 \\
             1 & 0 \\
             0 & 0 \\
             \vdots & \vdots \\
             0 & 0 \\
             0 & 1
          \end{pmatrix}
          \begin{pmatrix}
             w_{k, 1, 1} & \cdots & w_{k, 1, N} \\
             w_{k, 2, 1} & \cdots & w_{k, 2, N}
          \end{pmatrix}.
        </dt-math>
      </p>
      <p>
      Here is an example for a 3-dimensional <dt-math>\mathbf{x}</dt-math>
      and a 2-dimensional <dt-math>\mathbf{z}</dt-math> (reminder: this means
      a 4-dimensional <dt-math>\tilde{\mathbf{x}}</dt-math> and a 3-dimensional
      <dt-math>\tilde{\mathbf{z}}</dt-math>):
      </p>
      <figure class="l-body-outset figure" id="film-as-bilinear-diagram">
        <figcaption>
          Generalized bilinear weight matrices for a FiLM layer applied on a
          3-dimensional input <dt-math>\mathbf{x}</dt-math> within the context
          of a 2-dimensional <dt-math>\mathbf{z}</dt-math>. Grey elements
          have value <dt-math>0</dt-math>, white elements have value
          <dt-math>1</dt-math>, and color elements have arbitrary values.
        </figcaption>
      </figure>

      <h3>Gating as a generalized bilinear transformation</h3>
      <p>
        As explained earlier in the article, feature-wise gating layers can be
        seen as a bias-less FiLM layers with sigmoidal scaling. Representing
        them as a generalized bilinear transformation is a straightforward
        adaptation with respect to FiLM. Feature-wise gating is represented by
        an equation of the form

        <dt-math block>
          y_k = \alpha_k x_k, \quad \alpha_k \in [0, 1].
        </dt-math>

        This means that the context vector should have the form

        <dt-math block>
          \mathbf{v} = (0, \ldots, 0, \alpha_k, 0, \ldots, 0, 0)^T,
        </dt-math>

        which is achieved through a combination of a "one-hot sigmoid"
        nonlinearity

        <dt-math block>
          g(\mathbf{u}) = (u_0, \ldots, u_{k-1}, \sigma(u_k), u_{k+1}, \ldots, u_{N})^T,
        </dt-math>

        and a low-rank weight matrix <dt-math>W_k</dt-math> of the form

        <dt-math block>
          W_k =
          \begin{pmatrix}
             0 \\
             \vdots \\
             0 \\
             1 \\
             0 \\
             \vdots \\
             0
          \end{pmatrix}
          \begin{pmatrix}
             w_{k, 1} & \cdots & w_{k, N}
          \end{pmatrix}.
        </dt-math>
      </p>
      <p>
      Here is an example for a 3-dimensional <dt-math>\mathbf{x}</dt-math>
      and a 2-dimensional <dt-math>\mathbf{z}</dt-math> (reminder: this means
      a 4-dimensional <dt-math>\tilde{\mathbf{x}}</dt-math> and a 3-dimensional
      <dt-math>\tilde{\mathbf{z}}</dt-math>):
      </p>
      <figure class="l-body-outset figure" id="gating-as-bilinear-diagram">
        <figcaption>
          Generalized bilinear weight matrices for a gating layer applied on a
          3-dimensional input <dt-math>\mathbf{x}</dt-math> within the context
          of a 2-dimensional <dt-math>\mathbf{z}</dt-math>. Grey elements
          have value <dt-math>0</dt-math>, white elements have value
          <dt-math>1</dt-math>, and color elements have arbitrary values.
        </figcaption>
      </figure>

      <h3>Attention as a generalized bilinear transformation</h3>
      <p>
        A previous <a href="https://distill.pub/2016/augmented-rnns/">Distill article</a>
        already covers the subject of attention in great length, but here is a
        refresher for the benefit of this discussion.
      </p>
      <p>
        Attention is applied to a set of feature vectors at different spatial or
        temporal locations <dt-math>t \in [1, \cdots, T]</dt-math> by computing
        from internal or external conditioning information an
        <em>attention distribution</em>

        <dt-math block>
            \mathbf{p} = (p_1, \cdots, p_T)^T, \quad p^t > 0, \quad \sum_{t=1}^T p^t = 1.
        </dt-math>

        Assuming a feature matrix <dt-math>X</dt-math> in which columns
        represent feature vectors at different spatial or temporal locations,
        the output of the attentional interface is computed as the expected
        feature vector over the attention distribution:

        <dt-math block>
            \mathbf{y} = X \mathbf{p}.
        </dt-math>
      <p>
        Attention is closely related to gating in the generalized bilinear
        formulation, save from one crucial detail: because it implements a
        weighted sum over spatial or temporal locations, elements of the
        vector <dt-math>\mathbf{x}</dt-math> represent <em>the same feature
        at different spatial or temporal locations</em> rather than different
        features. It is represented by an equation of the form

        <dt-math block>
          y_k = \sum_{t=1}^T X_{k,t} p_t.
        </dt-math>

        This means that the context vector should have the form

        <dt-math block>
          \mathbf{v} = (p_1, \ldots, p_T, 0)^T,
        </dt-math>

        which is achieved through a combination of an "all-but-last softmax"
        nonlinearity

        <dt-math block>
          g(\mathbf{u}) = (\textrm{softmax}(\mathbf{u}_{1:T-1}), 0)^T,
        </dt-math>

        and a full-rank weight matrix <dt-math>W_k</dt-math>. Because the same
        weighted sum is applied to all features over spatial or temporal
        locations, matrices <dt-math>W_k</dt-math> are tied.
      </p>
      <figure class="l-body-outset figure" id="attention-as-bilinear-diagram">
        <figcaption>
          The generalized bilinear weight matrices for an attention layer are
          tied. Grey elements have value <dt-math>0</dt-math>, and color
          elements have arbitrary values.
        </figcaption>
      </figure>
      <p>
        Assuming a matrix
        <dt-math>X</dt-math> in which rows represent features and columns
        represent spatial or temporal locasions, we can represent the output
        <dt-math>\mathbf{y}</dt-math> compactly as

        <dt-math block>
          \mathbf{y} = \tilde{X} g(W\tilde{\mathbf{z}})
        </dt-math>

        where <dt-math>g(\cdot)</dt-math> is the "all-but-one" softmax
        introduced above.
      </p>

      <h3>Mixture-of-experts as a generalized bilinear transformation</h3>
      <p>
        Work on mixture-of-experts models
        <dt-cite key="jacobs1991adaptive,jordan1994hierarchical,eigen2014deep"></dt-cite>
        examines a framework in which the feature vectors output by several
        expert sub-networks are combined according to a mixture distribution
        which is itself computed from the experts' inputs.
      </p>
      <figure class="l-body-outset figure" id="moe-diagram">
        <figcaption>
            Mixture-of-experts model. The model's output is computed as a
            weighted sum of each expert's feature vector output as predicted
            by the gating network's mixture distribution output.
        </figcaption>
      </figure>
      <p>
        There is a strong connection between mixture-of-experts models and
        attention: the former can be viewed as attention over the experts'
        outputs rather than over time or space. As such, mixture-of-experts
        models can be represented as generalized bilinear transformations in the
        same way attention is, the only difference being that the columns of
        <dt-math>X</dt-math> represent experts rather than spatial or temporal
        locations.
      </p>
      <figure class="l-body-outset figure" id="moe-as-bilinear-diagram">
        <figcaption>
          Just like with attention, the generalized bilinear weight matrices for
          a mixture-of-experts model are tied. Grey elements have value
          <dt-math>0</dt-math>, and color elements have arbitrary values.
        </figcaption>
      </figure>

    </dt-article>

    <dt-appendix>
      <h3>Acknowledgements</h3>
      <p><span class="todo"> TODO: WRITEME.</span><p>

    <script type="text/bibliography">
      @inproceedings{vries2017modulating,
        author={de Vries, Harm and Strub, Florian and Mary, J\'{e}r\'{e}mie and
                Larochelle, Hugo and Pietquin, Olivier and Courville, Aaron},
        title={Modulating early visual processing by language},
        booktitle={Advances in Neural Information Processing Systems},
        year={2017},
        url={https://arxiv.org/pdf/1707.00683.pdf},
      }
      @inproceedings{vries2017guesswhat,
        title={GuessWhat?! Visual object discovery through multi-modal dialogue},
        author={de Vries, Harm and Strub, Florian and Chandar, Sarath and
                Pietquin, Olivier and Larochelle, Hugo and Courville, Aaron},
        booktitle={Proceedings of the Conference on Computer Vision and Pattern
                   Recognition},
        year={2017}
      }
      @inproceedings{perez2017learning,
        author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
                Vincent and Courville, Aaron},
        title={Learning visual reasoning without strong priors},
        booktitle={ICML Workshop on Machine Learning in Speech and Language Processing},
        year={2017},
        url={https://arxiv.org/pdf/1707.03017.pdf},
      }
      @inproceedings{perez2017film,
        author={Perez, Ethan and de Vries, Harm and Strub, Florian and Dumoulin,
                Vincent and Courville, Aaron},
        title={FiLM: Visual Reasoning with a General Conditioning Layer},
        booktitle={arXiv},
        year={2017},
        url={https://arxiv.org/pdf/1709.07871.pdf},
      }
      @inproceedings{johnson2017clevr,
        author={Johnson, Justin and Li, Fei-Fei and Hariharan, Bharath and Zitnick,
                Lawrence C. and van der Maaten, Laurens and Girshick, Ross},
        title={FiLM: Visual Reasoning with a General Conditioning Layer},
        booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition},
        year={2017},
        url={https://arxiv.org/pdf/1612.06890.pdf},
      }
      @inproceedings{dumoulin2017learned,
        author={Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath},
        title={A Learned Representation for Artistic Style},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2017},
        url={https://arxiv.org/pdf/1610.07629.pdf},
      }
      @inproceedings{ghiasi2017exploring,
        author={Ghiasi, Golnaz and Lee, Honglak and Kudlur, Manjunath and
                Dumoulin, Vincent and Shlens, Jonathon},
        title={Exploring the structure of a real-time, arbitrary neural artistic
               stylization network},
        booktitle={Proceedings of the British Machine Vision Conference},
        year={2017},
        url={https://arxiv.org/pdf/1705.06830.pdf},
      }
      @inproceedings{huang2017arbitrary,
        title={Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
        author={Huang, Xun and Belongie, Serge},
        booktitle={Proceedings of the International Conference on Computer Vision},
        year={2017},
        url={https://arxiv.org/pdf/1703.06868.pdf},
      }
      @inproceedings{radford2016unsupervised,
        author={Radford, Alec and Metz, Luke and Chintala, Soumith},
        title={Unsupervised Representation Learning with Deep Convolutional
               Generative Adversarial Networks},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2016},
        url={https://arxiv.org/pdf/1511.06434.pdf},
      }
      @inproceedings{oord2016conditional,
        title={Conditional Image Generation with PixelCNN Decoders},
        author={van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse
                and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
        booktitle={Advances in Neural Information Processing Systems},
        year={2016},
        url={https://arxiv.org/pdf/1606.05328.pdf}
      }
      @article{oord2016wavenet,
        title={WaveNet: A Generative Model for Raw Audio},
        author={van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and
                Simonyan, Karen and Vinyals, Oriol and Graves, Alex and
                Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
        journal={arXiv},
        year={2016},
        url={https://arxiv.org/pdf/1609.03499.pdf},
      }
      @inproceedings{ha2016hypernetworks,
        author={Ha, David and Dai, Andrew and Le, Quoc},
        title={HyperNetworks},
        booktitle={Proceedings of the International Conference on Learning Representations},
        year={2016},
        url={https://arxiv.org/pdf/1609.09106.pdf},
      }
      @inproceedings{kim2017dynamic,
        author={Kim, Taesup and Song, Inchul and Bengio, Yoshua},
        title={Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling
               in Speech Recognition},
        booktitle={Interspeech},
        year={2017},
        url={https://arxiv.org/pdf/1707.06065.pdf},
      }
      @inproceedings{hu2017squeeze,
        author={Hu, Jie and Shen, Li and Sun, Gang},
        title={Squeeze-and-Excitation Networks},
        booktitle={CVPR's ILSVRC 2017 Workshop},
        year={2017},
        url={https://arxiv.org/pdf/1709.01507.pdf},
      }
      @inproceedings{dauphin2017language,
        title={Language modeling with gated convolutional networks},
        author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
        booktitle={Proceedings of the International Conference on Machine Learning},
        year={2017},
        url={https://arxiv.org/pdf/1612.08083.pdf},
      }
      @inproceedings{gehring2017convolutional,
        author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats,
                Denis and Dauphin, Yann N.},
        title={Convolution Sequence-to-Sequence Learning},
        booktitle={Proceedings of the International Conference on Machine Learning},
        year={2017},
        url={https://arxiv.org/pdf/1705.03122.pdf},
      }
      @article{hochreiter1997long,
        title={Long Short-Term Memory},
        author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
        journal={Neural Computation},
        volume={9},
        number={8},
        pages={1735--1780},
        year={1997},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1997.9.8.1735},
      }
      @article{tenenbaum2000separating,
        title={Separating Style and Content with Bilinear Models},
        author={Tenenbaum, Joshua B. and Freeman, William T.},
        journal={Neural Computation},
        volume={12},
        number={6},
        pages={1247--1283},
        year={2000},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/089976600300015349},
      }
      @article{dhingra2017gated,
        title={Gated-Attention Readers for Text Comprehension},
        author={Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin, and
                Cohen, William W and Salakhutdinov, Ruslan},
        booktitle={Proceedings of the Annual Meeting of the Association for
                   Computational Linguistics},
        year={2017},
        url={https://arxiv.org/pdf/1606.01549.pdf},
      }
      @inproceedings{chaplot2017gated,
        title={Gated-Attention Architectures for Task-Oriented Language Grounding},
        author={Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and
                Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov,
                Ruslan},
        booktitle={ACL Workshop on Language Grounding for Robotics},
        year={2017},
        url={https://arxiv.org/pdf/1706.07230.pdf},
      }
      @inproceedings{shen2017disan,
        title={DiSAN: Directional Self-Attention Network for RNN/CNN-free
               Language Understanding},
        author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and
                Pan, Shirui and Zhang, Chengqi},
        booktitle={arXiv},
        year={2017},
        url={https://arxiv.org/pdf/1709.04696.pdf},
      }
      @inproceedings{bowman2015large,
        title={A large annotated corpus for learning natural language inference},
        author={Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and
                Manning, Christopher D.},
        booktitle={Proceedings of the Conference on Empirical Methods in Natural
                   Language Processing},
        year={2015},
        url={https://nlp.stanford.edu/pubs/snli_paper.pdf},
      }
      @article{jacobs1991adaptive,
        title={Adaptive Mixtures of Local Experts},
        author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J.
                and Hinton, Geoffrey E.},
        journal={Neural Computation},
        volume={3},
        number={1},
        pages={79--87},
        year={1991},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1991.3.1.79},
      }
      @article{jordan1994hierarchical,
        title={Hierarchical Mixtures of Experts and the EM Algorithm},
        author={Jordan, Michael I. and Jacobs, Robert A.},
        journal={Neural Computation},
        volume={6},
        number={2},
        pages={181--214},
        year={1994},
        publisher={MIT Press},
        url={http://dx.doi.org/10.1162/neco.1994.6.2.181},
      }
      @inproceedings{eigen2014deep,
        author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
        title={Learning Factored Representations in a Deep Mixture of Experts},
        booktitle={Proceedings of the ICLR Workshops},
        year={2014},
        url={https://arxiv.org/pdf/1312.4314.pdf},
      }
      @article{kirkpatrick2017overcoming,
        title={Overcoming catastrophic forgetting in neural networks},
        author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and
                Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and
                Milan, Kieran and Quan, John and Ramalho, Tiago and
                Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath,
                Claudia and Kumaran, Dharshan and Hadsell, Raia},
        journal={Proceedings of the National Academy of Sciences},
        volume={114},
        number={13},
        pages={3521--3526},
        year={2017},
        url={http://www.pnas.org/content/114/13/3521.abstract},
      }
      @inproceedings{mel1992clusteron,
        title={The Clusteron: Toward a Simple Abstraction for a Complex Neuron},
        author={Mel, Bartlett W},
        booktitle={Advances in Neural Information Processing Systems},
        year={1992},
        url={http://papers.nips.cc/paper/450-the-clusteron-toward-a-simple-abstraction-for-a-complex-neuron.pdf},
      }
      @article{boutonnet2015words,
        title={Words jump-start vision: A label advantage in object recognition},
        author={B. Boutonnet and G. Lupyan},
        journal={Journal of Neuroscience},
        volume={35},
        number={25},
        pages={9329--9335},
        year={2015},
        publisher={Society for Neuroscience},
        url={},
      }
    </script>
    <script src="assets/figure_film.js"></script>
    <script src="assets/figure_film_architecture.js"></script>
    <script src="assets/figure_bilinear.js"></script>
    <script src="assets/figure_generalized_bilinear.js"></script>
    <script src="assets/figure_film_as_bilinear.js"></script>
    <script src="assets/figure_gating_as_bilinear.js"></script>
    <script src="assets/figure_attention_as_bilinear.js"></script>
    <script src="assets/figure_moe_as_bilinear.js"></script>
    <script src="assets/figure_clevr.js"></script>
    <script src="assets/figure_guesswhat.js"></script>
    <script src="assets/figure_alrfas.js"></script>
    <script src="assets/figure_adain.js"></script>
    <script src="assets/figure_dcgan.js"></script>
    <script src="assets/figure_pixelcnn.js"></script>
    <script src="assets/figure_dln.js"></script>
    <script src="assets/figure_squeeze.js"></script>
    <script src="assets/figure_convseq2seq.js"></script>
    <script src="assets/figure_gated_attention.js"></script>
    <script src="assets/figure_disan.js"></script>
    <script src="assets/figure_moe.js"></script>
    <script src="assets/figure_rl.js"></script>
    <script src="assets/figure_clevr_tsne.js"></script>
  </body>
</html>
