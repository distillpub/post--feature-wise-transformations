<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script src="https://d3js.org/d3-selection-multi.v1.min.js"></script>
  <script src="http://ariutta.github.io/svg-pan-zoom/dist/svg-pan-zoom.js"></script>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
    "title": "Understanding FiLM",
    "description": "Description of the post",
    "authors": [
      {
        "author": "Vincent Dumoulin",
        "authorURL": "https://vdumoulin.github.io",
        "affiliations": [{"name": "Google Brain", "url": "https://ai.google/research/teams/brain"}]
      },
      {
        "author": "Ethan Perez",
        "authorURL": "http://ethanperez.net/",
        "affiliations": [
          {"name": "Rice University", "url": "http://www.rice.edu/"},
          {"name": "MILA", "url": "https://mila.quebec/en/"}
        ]
      },
      {
        "author": "Florian Strub",
        "authorURL": "https://fstrub95.github.io/",
        "affiliations": [{"name": "Univ. of Lille, Inria", "url": "https://team.inria.fr/sequel/"}]
      },
      {
        "author": "Harm de Vries",
        "authorURL": "http://www-etud.iro.umontreal.ca/~devries/",
        "affiliations": [{"name": "MILA", "url": "https://mila.quebec/en/"}]
      },
      {
        "author": "Nathan Schucher",
        "authorURL": "https://nathanschucher.com/",
        "affiliations": [{"name": "Element AI", "url": "https://element.ai/"}]
      },
      {
        "author": "Aaron Courville",
        "authorURL": "https://aaroncourville.wordpress.com/",
        "affiliations": [{"name": "MILA", "url": "https://mila.quebec/en/"}]
      },
      {
        "author": "Yoshua Bengio",
        "authorURL": "http://www.iro.umontreal.ca/~bengioy/yoshua_en/",
        "affiliations": [{"name": "MILA", "url": "https://mila.quebec/en/"}]
      }
    ],
    "katex": {
      "delimiters": [
        {
          "left": "$",
          "right": "$",
          "display": false
        },
        {
          "left": "$$",
          "right": "$$",
          "display": true
        }
      ]
    }
  }</script>
</d-front-matter>

<d-title>
  <h1>Feature-wise transformations</h1>
  <p>A family of methods for fusing multiple sources of information</p>
  <div class="l-page" id="vtoc"></div>
</d-title>

<d-article>
  <p>
    Many real-world problems require integrating multiple sources of information.
    Sometimes these problems involve multiple, distinct modalities of
    information &mdash; vision, language, audio, etc. &mdash; as is required
    to understand a scene in a movie or answer a question about an image.
    Other times, these problems involve multiple sources of the same
    kind of input, i.e. when summarizing several documents or drawing one
    image in the style of another.
  </p>
  <figure class="l-body-outset">
    <%= require("../static/diagrams/multimodal.svg") %>
  </figure>
  <p>
    When approaching such problems, it often makes sense to process one source
    of information <em>in the context of</em> another; for instance, in the
    right example above, one can extract meaning from the image
    in the context of the question asked to answer it correctly. In machine
    learning, we often refer to this context-based processing as
    <em>conditioning</em>: the computation carried out by a model is conditioned
    or <em>modulated</em> by information extracted from an auxiliary input.
  </p>
  <p>
    Finding an effective way to condition on or fuse sources of information
    is an open research problem, and
    <!-- Introduction -->
    in this article, we concentrate on a specific family of approaches we call
    <em>feature-wise transformations</em>.
    <!-- Related Work -->
    We will examine the use of feature-wise transformations in many neural network
    architectures to solve a surprisingly large and diverse set of problems;
    <!-- Experiments -->
    their success, we will argue, is due to being flexible enough to learn an
    effective representation of the conditioning input in varied settings.
    In the language of multi-task learning, where the conditioning signal is
    taken to be a task description, feature-wise transformations
    learn a task representation which allows them to capture and leverage the
    relationship between multiple sources of information, even in remarkably
    different problem settings.
  </p>

  <hr/>

  <h2>Feature-wise transformations</h2>
  <p>
    To motivate feature-wise transformations, we start with a basic example,
    where the two inputs are images and category labels, respectively.
    For the purpose of this example, we
    are interested in building a generative model of various classes of images
    (puppy, boat, airplane, etc.). The model takes as input a class and a source
    of random noise (i.e., a vector sampled from a normal distribution)
    and outputs an image sample for the requested class.
  </p>
  <figure class="l-body">
    <%= require("../static/diagrams/conditional-generation.svg") %>
  </figure>
  <p>
    Our first instinct might be to build a separate model for each
    class. For a small number of classes this approach is not too bad a solution,
    but for thousands of classes, we quickly run into scaling issues, as the number
    of parameters to store and train grows with the number of classes.
    We are also missing out on the opportunity to leverage commonalities between
    classes; for instance, different types of dogs (puppy, terrier, dalmatian,
    etc.) share visual traits and are likely to share computation when
    mapping from the abstract noise vector to the output image.
  </p>
  <p>
    Now let's imagine that, in addition to the various classes, we also need to
    model attributes like size or color. In this case, we can't
    reasonably expect to train a separate network for <em>each</em> possible
    conditioning combination! Let's examine a few simple options.
  </p>
  <p>
    A quick fix would be to concatenate a representation of the conditioning
    information to the noise vector and treat the result as the model's input.
    This solution is quite parameter-efficient, as we only need to increase
    the size of the first layer's weight matrix. However, this approach makes the implicit
    assumption that the input is where the model needs to use the conditioning information.
    Maybe this assumption is correct, or maybe it's not; perhaps the
    model does not need to incorporate the conditioning information until late
    into the generation process. In this case, we would be forcing the model to
    carry this information around unaltered for many layers.
  </p>
  <p>
    Because this operation is cheap, we might as well avoid making any such
    assumptions and concatenate the conditioning representation to the input of
    <em>all</em> layers in the network. Let's call this approach
    <em>concatenation-based conditioning</em>.
  </p>
  <figure class="l-body">
    <%= require("../static/diagrams/concatenation-based-conditioning.svg") %>
  </figure>
  <p>
    Another efficient way to integrate conditional information into the network
    is via <em>conditional biasing</em>, namely, by adding a <em>bias</em> to
    the hidden layers based on the conditioning representation.
  </p>
  <figure class="l-body">
    <%= require("../static/diagrams/conditional-biasing.svg") %>
  </figure>
  <p>
    Interestingly, conditional biasing is actually just another way to implement
    concatenation-based conditioning. Consider a fully-connected linear layer
    applied to a concatenated an input <d-math>\mathbf{x}</d-math> and
    conditioning representation <d-math>\mathbf{z}</d-math>:
    <d-footnote>
      The same argument applies to convolutional networks, provided we ignore
      the border effects due to zero-padding.
    </d-footnote>
  </p>
  <figure class="l-body">
    <%= require("../static/diagrams/concatenation-as-biasing.svg") %>
  </figure>
  <p>
    Yet another efficient way to integrate class information into the network is
    via <em>conditional scaling</em>, i.e., scaling hidden layers
    based on the conditioning representation.
  </p>
  <figure class="l-body">
    <%= require("../static/diagrams/conditional-scaling.svg") %>
  </figure>
  <p>
    A special instance of conditional scaling is feature-wise sigmoidal gating:
    scaled each feature by a value between <d-math>0</d-math> and
    <d-math>1</d-math> (enforced by applying the sigmoid function), as a
    function of the conditioning representation. Intuitively, this gating allows
    the conditioning information to select which features are passed forward
    and which are zeroed out.
  </p>
  <p>
    Given that both additive and multiplicative interactions seem natural and
    intuitive, which approach should we pick? One argument in favor of
    <em>multiplicative</em> interactions is that they are useful in learning
    relationships between inputs, as these interactions naturally identify
    "matches": elements with similar values (regardless of sign) will
    generally produce higher values when multiplied and lower values otherwise.
    This property is why dot products are often used to determine how similar
    two vectors are.
    <d-footnote>
      Multiplicative interactions alone have had a history of success in various
      domains &mdash; see <a href="#bibliographic-notes">Bibliographic Notes</a>.
    </d-footnote>
    One argument in favor of <em>additive</em> interactions is that they are
    more natural for applications that are less strongly dependent on the
    joint values of two inputs, like feature aggregation or feature detection
    (i.e., checking if a feature is present in either of two inputs).
  </p>
  <p>
    In the spirit of making as few assumptions about the problem as possible,
    we may as well combine <em>both</em> into a
    conditional <em>affine transformation</em>.
    <d-footnote>
      An affine transformation is a transformation of the form
      <d-math>y = m * x + b</d-math>.
    </d-footnote>
    There is some evidence to suggest that using both kinds of interactions
    is more effective than either alone
    <d-cite key="perez2018film,yu2018guided"></d-cite>,
    perhaps from getting the best of both worlds.
  </p>
  <p>
    All methods outlined above share the common trait that they act at the
    <em>feature</em> level; in other words, they leverage <em>feature-wise</em>
    interactions between the conditioning representation and the conditioned
    network. It is certainly possible to use more complex interactions,
    but feature-wise interactions often strike a happy compromise between
    effectiveness and efficiency: the number of conditioning scaling and/or
    shifting coefficients to predict scales linearly with the number of features
    in the network. Also, in practice, feature-wise transformations (often
    compounded across multiple layers) frequently have enough capacity to
    model complex phenomenon in various settings.
  </p>
  <p>
    Lastly, note how limited an inductive bias these transformations provide
    &mdash; how general their motivation is. This quality can be a downside, as
    some problems may be easier to solve with a stronger inductive bias.
    However, it is this characteristic which also enables these transformations
    to be so widely effective across problem domains, as we will review later
    on.
  </p>
  <h3>Nomenclature</h3>
  <p>
    To continue the discussion on feature-wise transformations we need to
    abstract away the distinction between multiplicative and additive
    interactions. Without losing generality, let's focus on feature-wise affine
    transformations, and let's adopt the nomenclature of Perez et al.
    <d-cite key="perez2018film"></d-cite>, which formalizes conditional affine
    transformations under the acronym <em>FiLM</em>, for Feature-wise Linear
    Modulation.
    <d-footnote>
      Strictly speaking, <em>linear</em> is a misnomer, as we allow biasing, but
      we hope the more rigorous-minded reader will forgive us for the sake of a
      better-sounding acronym.
    </d-footnote>
  </p>
  <p>
    We say that a neural network is modulated using FiLM, or <em>FiLM-ed</em>,
    after inserting <em>FiLM layers</em> into its architecture. These layers are
    parametrized by some form of conditioning information, and the mapping from
    conditioning information to FiLM parameters is called the <em>FiLM generator</em>.
    In other words, the FiLM generator predicts the parameters of the FiLM
    layers based on some auxiliary input. For simplicity, you can assume that
    the FiLM generator outputs the concatenation of all FiLM parameters for the
    network architecture.
  </p>
  <figure class="l-body">
    <%= require("../static/diagrams/film-architecture.svg") %>
  </figure>
  <p>
    As the name implies, a FiLM layer applies a feature-wise affine
    transformation to its input. By <em>feature-wise</em>, we mean that scaling
    and shifting are applied element-wise, or in the case of convolutional
    networks, feature map -wise.
    <d-footnote>
      To expand a little more on the convolutional case, feature maps can be
      thought of as the same feature detector being evaluated at different
      spatial locations, in which case it makes sense to apply the same affine
      transformation to all spatial locations.
    </d-footnote>
    In other words, assuming <d-math>\mathbf{x}</d-math> is a FiLM layer's
    input, <d-math>\mathbf{z}</d-math> is conditioning input, and
    <d-math>\gamma</d-math> and <d-math>\beta</d-math> are
    <d-math>\mathbf{z}</d-math>-dependent scaling and shifting vectors,

    <d-math block>
        \textrm{FiLM}(\mathbf{x}) = \gamma(\mathbf{z}) \odot \mathbf{x}
                                                       + \beta(\mathbf{z}).
    </d-math>

    You can interact with the following fully-connected and convolutional FiLM
    layers to get an intuition of the sort of modulation they allow:
  </p>
  <figure class="l-body-outset" id="film-layer-diagram">
    <%= require("../static/diagrams/film-layer.svg") %>
  </figure>
  <p>
    In addition to being a good abstraction of conditional feature-wise
    transformations, the FiLM nomenclature lends itself well to the notion of a
    <em>task representation</em>. From the perspective of multi-task learning,
    we can view the conditioning signal as the task description, and we can view the
    concatenation of all FiLM scaling and shifting coefficients as both
    an instruction on <em>how to modulate</em> the conditioned network and a
    <em>representation</em> of the task at hand. We will explore and illustrate
    this idea later on.
  </p>

  <hr/>

  <h2>Feature-wise transformations in the literature</h2>
  <p>
    Feature-wise transformations find their way into methods in many problem
    settings, but because of their simplicity, their effectiveness is
    seldom highlighted in lieu of other novel research contributions.
    Below are a few notable examples of feature-wise transformations in the
    literature, grouped by application domain. The diversity of these
    applications underscores the flexible, general-purpose ability of
    feature-wise interactions to learn effective task representations.
  </p>
  <div style="width: 100%">
    <button class="expand-collapse-button" content-type="literature">expand all</button>
  </div>
  <button class="collapsible" content-name="generative-modeling" content-type="literature">Generative modeling<span style="float: right;">+</span></button>
  <p class="content" content-name="generative-modeling" content-type="literature">
    The conditional variant of DCGAN <d-cite key="radford2016unsupervised"></d-cite>,
    a well-recognized class of network architectures for generative adversarial networks
    <d-cite key="goodfellow2014generative"></d-cite>, uses concatenation-based
    conditioning. The class label is broadcasted as a feature map and then
    concatenated to the input of convolutional and transposed convolutional
    layers in the discriminator and generator networks.
  </p>
  <figure class="content l-body" content-name="generative-modeling" content-type="literature">
    <%= require("../static/diagrams/dcgan.svg") %>
  </figure>
  <p class="content" content-name="generative-modeling" content-type="literature">
    PixelCNN <d-cite key="oord2016conditional"></d-cite>
    and WaveNet <d-cite key="oord2016wavenet"></d-cite> are two recent advances
    in autoregressive, generative modeling (of images and audio,
    respectively) which use conditional biasing. The simplest form of
    conditioning in PixelCNN adds feature-wise biases to all convolutional layer
    outputs. In FiLM parlance, this operation is equivalent to
    inserting FiLM layers after each convolutional layer and setting the
    scaling coefficients to a constant value of 1.
    <d-footnote>
      The authors also describe a location-dependent biasing scheme which
      cannot be expressed in terms of FiLM layers due to the absence of the
      feature-wise property.
    </d-footnote>
  </p>
  <figure class="content l-body" content-name="generative-modeling" content-type="literature">
    <%= require("../static/diagrams/pixelcnn.svg") %>
  </figure>
  <p class="content" content-name="generative-modeling" content-type="literature">
    WaveNet describes two ways in which conditional biasing allows external
    information to modulate the audio or speech generation process based on
    conditioning input:
  </p>
  <ol class="content" content-name="generative-modeling" content-type="literature">
    <li>
      <strong>Global conditioning</strong> applies the same conditional bias
      to the whole generated sequence and is used e.g. to condition on speaker
      identity.
    </li>
    <li>
      <strong>Local conditioning</strong> applies a conditional bias which
      varies across time steps of the generated sequence and is used e.g. to
      let linguistic features in a text-to-speech model influence which sounds
      are produced.
    </li>
  </ol>
  <p class="content" content-name="generative-modeling" content-type="literature">
    As in PixelCNN, conditioning in WaveNet can be viewed as inserting FiLM
    layers after each convolutional layer. The main difference lies in how
    the FiLM-generating network is defined: global conditioning
    expresses the FiLM-generating network as an embedding lookup which is
    broadcasted to the whole time series, whereas local conditioning expresses
    it as a mapping from an input sequence of conditioning information to an
    output sequence of FiLM parameters.
  </p>
  <button class="collapsible" content-name="reinforcement-learning" content-type="literature">Reinforcement learning<span style="float: right;">+</span></button>
  <p class="content" content-name="reinforcement-learning" content-type="literature">
    The Gated-Attention architecture <d-cite key="chaplot2017gated"></d-cite>
    uses feature-wise sigmoidal gating to fuse linguistic and visual
    information in an agent trained to follow simple "go-to" language
    instructions in the VizDoom <d-cite key="kempa2016vizdoom"></d-cite> 3D
    environment.
  </p>
  <figure class="content l-body" content-name="reinforcement-learning" content-type="literature">
    <%= require("../static/diagrams/gated-attention.svg") %>
  </figure>
  <p class="content" content-name="reinforcement-learning" content-type="literature">
    Bahdanau et al.<d-cite key="bahdanau2018learning"></d-cite> use FiLM
    layers to condition Neural Module Network<d-cite key="andreas2016neural"></d-cite>
    and LSTM<d-cite key="hochreiter1997long"></d-cite> -based policies to follow
    basic, compositional language instructions (arranging objects and going
    to particular locations) in a 2D grid world. They train this policy
    in an adversarial manner using rewards from another FiLM-based network,
    trained to discriminate between ground-truth examples of achieved
    instruction states and failed policy trajectories states.
  </p>
  <p class="content" content-name="reinforcement-learning" content-type="literature">
    Outside instruction-following, Kirkpatrick et al.
    <d-cite key="kirkpatrick2017overcoming"></d-cite> also use
    game-specific scaling and biasing to condition a shared policy network
    trained to play 10 different Atari games.
  </p>
  <button class="collapsible" content-name="nlp" content-type="literature">Natural language processing<span style="float: right;">+</span></button>
  <p class="content" content-name="nlp" content-type="literature">
    The Gated-Attention Reader <d-cite key="dhingra2017gated"></d-cite>
    uses feature-wise scaling, extracting information
    from text by conditioning a document reading network on a query. Its
    architecture consists of multiple Gated-Attention modules, which involve
    element-wise multiplications between document representation tokens and
    token-specific query representations extracted via soft attention on the
    query representation tokens.
  </p>
  <figure class="content l-body" content-name="nlp" content-type="literature">
    <%= require("../static/diagrams/gated-attention-reader.svg") %>
  </figure>
  <p class="content" content-name="nlp" content-type="literature">
    Dauphin et al. <d-cite key="dauphin2017language"></d-cite> use sigmoidal
    gating in their proposed gated linear
    unit, which uses one half of the input features to apply feature-wise
    sigmoidal gating to the other half. Gehring et al.
    <d-cite key="gehring2017convolutional"></d-cite> adopt this
    architectural feature, introducing a fast, parallelizable model for
    machine translation in the form of a fully convolutional network.
  </p>
  <figure class="content l-body" content-name="nlp" content-type="literature">
    <%= require("../static/diagrams/convseq2seq.svg") %>
  </figure>
  <button class="collapsible" content-name="vqa" content-type="literature">Visual question-answering<span style="float: right;">+</span></button>
  <p class="content" content-name="vqa" content-type="literature">
    Perez et al. <d-cite key="perez2017learning,perez2018film"></d-cite> use
    FiLM layers to build a visual reasoning model
    trained on the CLEVR dataset <d-cite key="johnson2017clevr"></d-cite> to
    answer multi-step, compositional questions about synthetic images.
  </p>
  <figure class="content l-body-outset" content-name="vqa" content-type="literature">
    <%= require("../static/diagrams/film-clevr.svg") %>
  </figure>
  <p class="content" content-name="vqa" content-type="literature">
    The model's linguistic pipeline is a FiLM generator which
    extracts a question representation that is linearly mapped to
    FiLM parameter values. Using these values, FiLM layers inserted within each
    residual block condition the visual pipeline. The model is trained
    end-to-end on image-question-answer triples.
  </p>
  <p class="content" content-name="vqa" content-type="literature">
    de Vries et al. <d-cite key="vries2017modulating"></d-cite> leverage FiLM
    to condition a pre-trained network. Their model's linguistic pipeline
    modulates the visual pipeline via conditional batch normalization,
    which can be viewed as a special case of FiLM. The model learns to answer natural language questions about
    real-world images on the GuessWhat?! <d-cite key="vries2017guesswhat"></d-cite>
    and VQAv1 <d-cite key="agrawal2015vqa"></d-cite> datasets.
  </p>
  <figure class="content l-body-outset" content-name="vqa" content-type="literature">
    <%= require("../static/diagrams/guesswhat.svg") %>
  </figure>
  <p class="content" content-name="vqa" content-type="literature">
    The visual pipeline consists of a pre-trained residual network that is
    fixed throughout training. The linguistic pipeline manipulates the visual
    pipeline by perturbing the residual network's batch normalization
    parameters, which re-scale and re-shift feature maps after activations
    have been normalized to have zero mean and unit variance.  As hinted
    earlier, conditional batch normalization can be viewed as an instance of
    FiLM where the post-normalization feature-wise affine transformation is
    replaced with a FiLM layer.
  </p>
  <button class="collapsible" content-name="style-transfer" content-type="literature">Style transfer<span style="float: right;">+</span></button>
  <p class="content" content-name="style-transfer" content-type="literature">
    Dumoulin et al. <d-cite key="dumoulin2017learned"></d-cite> use
    feature-wise affine transformations &mdash; in the form of conditional
    instance normalization layers &mdash; to condition a style transfer
    network on a chosen style image. Like conditional batch normalization
    discussed in the previous subsection,
    conditional instance normalization can be seen as an instance of FiLM
    where a FiLM layer replaces the post-normalization feature-wise affine
    transformation. For style transfer, the network models each style as a separate set of
    instance normalization parameters, and it applies normalization with these
    style-specific parameters.
  </p>
  <figure class="content l-body-outset" content-name="style-transfer" id="alrfas-diagram" content-type="literature">
    <%= require("../static/diagrams/alrfas.svg") %>
  </figure>
  <p class="content" content-name="style-transfer" content-type="literature">
    Dumoulin et al. <d-cite key="dumoulin2017learned"></d-cite> use a simple
    embedding lookup to produce instance normalization parameters, while
    Ghiasi et al. <d-cite key="ghiasi2017exploring"></d-cite> further
    introduce a <em>style prediction network</em>, trained jointly with the
    style transfer network to predict the conditioning parameters directly
    from a given style image.
    Though for this article, we prefer FiLM as an abstraction because it is
    decoupled from any normalization operations, FiLM layers were themselves
    heavily inspired by conditional normalization layers.
  </p>
  <p class="content" content-name="style-transfer" content-type="literature">
    Yang et al. <d-cite key="yang2018efficient"></d-cite> use a related
    architecture for video object segmentation, the task of segmenting a
    particular object throughout a video, given that object's segmentation in
    the first frame.  Their model conditions an image segmentation network
    over a video frame on the provided first frame segmentation using
    feature-wise gains, as well as on the previous frame using position-wise
    biases.
  </p>
  <p class="content" content-name="style-transfer" content-type="literature">
    So far, many of the models we covered have two sub-networks: a primary
    network in which feature-wise transformations are applied and a secondary
    network which outputs parameters for these transformations. However, this
    distinction between <em>FiLMed network</em> and <em>FiLM generator</em>
    is not strictly necessary. As an example, Huang and Belongie
    <d-cite key="huang2017arbitrary"></d-cite> propose an alternative
    style transfer network that uses adaptive instance normalization layers,
    which compute normalization parameters using a simple heuristic.
  </p>
  <figure class="content l-body-outset" content-name="style-transfer" id="adain-diagram" content-type="literature">
    <%= require("../static/diagrams/adain.svg") %>
  </figure>
  <p class="content" content-name="style-transfer" content-type="literature">
    Adaptive instance normalization can be interpreted as inserting a FiLM
    layer midway through the model. However, rather than relying
    on a secondary network to predict the FiLM parameters from the style
    image, the main network itself is used to extract the style features
    used to compute FiLM parameters. Therefore, the model can be seen as
    <em>both</em> the FiLMed network the FiLM generator.
  </p>
  <button class="collapsible" content-name="domain-adaptation" content-type="literature">Domain adaptation<span style="float: right;">+</span></button>
  <p class="content" content-name="domain-adaptation" content-type="literature">
    Li et al. <d-cite key="li2018adaptive"></d-cite> find that
    updating the per-channel batch
    normalization statistics (mean and variance) of a network trained on one
    domain with that network's statistics in a new, target domain. Updating
    these statistics is mathematically equivalent to modifying the
    feature-wise affine transformations in batch normalization layers, as in
    conditional normalization layers discussed in the "Visual question-answering"
    and "Style transfer" subsections. Notably, this approach, along with
    Adaptive Instance Normalization, has the particular advantage of not
    requiring any extra, trainable parameters.
  </p>
  <button class="collapsible" content-name="image-recognition" content-type="literature">Image recognition<span style="float: right;">+</span></button>
  <p class="content" content-name="image-recognition" content-type="literature">
    Many models thus far condition on a separate source of conditioning
    information, but there is nothing preventing us from considering a
    neural network's hidden activations <em>themselves</em> as conditioning
    information. This idea gives rise to
    <em>self-conditioned</em> models.
  </p>
  <figure class="content l-body" content-name="image-recognition" content-type="literature">
    <%= require("../static/diagrams/self-conditioning.svg") %>
  </figure>
  <p class="content" content-name="image-recognition" content-type="literature">
    Highway Networks <d-cite key="srivastava2015highway"></d-cite> are a prime
    example of applying this principle of self-conditioning. This network
    architecture takes inspiration from the LSTMs'
    <d-cite key="hochreiter1997long"></d-cite> heavy use of feature-wise
    sigmoidal gating in their input, forget, and output gates to regulate
    information flow:
  </p>
  <figure class="content l-body" content-name="image-recognition" content-type="literature">
    <%= require("../static/diagrams/highway-networks.svg") %>
  </figure>
  <p class="content" content-name="image-recognition" content-type="literature">
    The ImageNet 2017 winning model <d-cite key="hu2017squeeze"></d-cite> also
    employs feature-wise sigmoidal gating in a self-conditioning manner, as a
    way to "recalibrate" a layer's activations conditioned on themselves.
  </p>
  <figure class="content l-body" content-name="image-recognition" content-type="literature">
    <%= require("../static/diagrams/squeeze-and-excitation.svg") %>
  </figure>
  <button class="collapsible" content-name="speech-recognition" content-type="literature">Speech recognition<span style="float: right;">+</span></button>
  <p class="content" content-name="speech-recognition" content-type="literature">
    Kim et al.<d-cite key="kim2017dynamic"></d-cite> modulate a deep
    bidirectional LSTM using <em>dynamic layer normalization</em>, a form
    of conditional normalization as discussed in the "Visual question-answering"
    and "Style transfer" subsections.
    Dynamic layer normalization can also be seen as an instance of FiLM where the
    post-normalization feature-wise affine transformation is disabled and a
    FiLM layer is placed immediately after the normalization.
  </p>
  <figure class="content l-body-outset" content-name="speech-recognition" content-type="literature">
    <%= require("../static/diagrams/dln.svg") %>
  </figure>
  <p class="content" content-name="speech-recognition" content-type="literature">
    The key difference here is that the conditioning signal does not come from
    an external source but rather from utterance
    summarization feature vectors extracted in each layer to adapt the model.
  </p>

  <hr/>

  <h2>Related ideas</h2>
  <p>
    Aside from methods which directly use feature-wise transformations, FiLM
    connects more broadly with other methods and concepts as well.
  </p>
  <div style="width: 100%">
    <button class="expand-collapse-button" content-type="related">expand all</button>
  </div>
  <button class="collapsible" content-name="zero-shot" content-type="related">Zero-shot learning<span style="float: right;">+</span></button>
  <p class="content" content-name="zero-shot" content-type="related">
    The idea of learning a task representation shares a strong connection with
    approaches in zero-shot learning.
    In zero-shot learning, semantic task embeddings may be learned from
    external information and then leveraged to make predictions about classes
    without training examples. For instance, to
    generalize to unseen object categories for image classification,
    one may construct semantic task embeddings
    from text-only descriptions and exploit objects' text-based relationships
    to predict unseen image categories. Frome et al.
    <d-cite key="frome2013devise"></d-cite>, Socher et al.
    <d-cite key="socher2013zero"></d-cite>, and Norouzi et al.
    <d-cite key="norouzi2014zero"></d-cite> are a few notable exemplars
    of this idea.
  </p>
  <button class="collapsible" content-name="hypernetworks" content-type="related">HyperNetworks<span style="float: right;">+</span></button>
  <p class="content" content-name="hypernetworks" content-type="related">
    The notion of a secondary network predicting the parameters of a primary
    network is also well exemplified by HyperNetworks
    <d-cite key="ha2016hypernetworks"></d-cite>, which predict weights
    describing entire layers (e.g., in a recurrent neural network).
    From this perspective, the FiLM
    generator is a specialized HyperNetwork that
    predicts the FiLM parameters of the FiLMed network. The main distinction
    between the two resides in the number and specificity of predicted
    parameters: FiLM requires predicting far fewer parameters than
    Hypernetworks, but FiLM also has less capacity to modulate
    other networks. The ideal trade-off between capacity and regularization (and
    computational complexity) in using conditioning information is still an
    ongoing area of investigation, and many proposed approaches lie on the
    spectrum between FiLM and HyperNetworks (see
    <a href="#bibliographic-notes">Bibliographic Notes</a>).
  </p>
  <button class="collapsible" content-name="attention" content-type="related">Attention<span style="float: right;">+</span></button>
  <p class="content" content-name="attention" content-type="related">
    Some parallels can be drawn between attention and FiLM, but the two operate
    in different ways which are important to disambiguate.
  </p>
  <figure class="content l-body-outset" content-name="attention" content-type="related" id="film-vs-attention-diagram">
    <%= require("../static/diagrams/film-vs-attention.svg") %>
  </figure>
  <p class="content" content-name="attention" content-type="related">
    This difference stems from distinct intuitions underlying attention and
    FiLM: the former assumes that specific spatial locations or time steps
    contain the most useful information, whereas the latter assumes that
    specific features or feature maps contain the most useful information.
  </p>
  <button class="collapsible" content-name="bilinear" content-type="related">Bilinear transformations<span style="float: right;">+</span></button>
  <p class="content" content-name="bilinear" content-type="related">
    With a little bit of stretching, FiLM can be seen as a special case of a
    bilinear transformation
    <d-cite key="tenenbaum2000separating"></d-cite> with low-rank weight
    matrices. A bilinear transformation defines the relationship between two
    inputs <d-math>\mathbf{x}</d-math> and <d-math>\mathbf{z}</d-math> and the
    <d-math>k^{th}</d-math> output feature <d-math>y_k</d-math> as

    <d-math block>
      y_k = \mathbf{x}^T W_k \mathbf{z}.
    </d-math>

    Note that for each output feature <d-math>y_k</d-math> we have a separate
    matrix <d-math>W_k</d-math>, so the full set of weights forms a
    multi-dimensional array.
  </p>
  <figure class="content l-body-outset" content-name="bilinear" content-type="related">
    <%= require("../static/diagrams/bilinear.svg") %>
  </figure>
  <p class="content" content-name="bilinear" content-type="related">
    If we view <d-math>\mathbf{z}</d-math> as the concatenation of the scaling
    and shifting vectors <d-math>\gamma</d-math> and <d-math>\beta</d-math> and
    if we augment the input <d-math>\mathbf{x}</d-math> with a 1-valued feature,
    <d-footnote>
      As is commonly done to turn a linear transformation into an affine
      transformation.
    </d-footnote>
    we can represent FiLM using a bilinear transformation by zeroing out the
    appropriate weight matrix entries:
  </p>
  <figure class="content l-body-outset" content-name="bilinear" content-type="related">
    <%= require("../static/diagrams/film-as-bilinear.svg") %>
  </figure>
  <p class="content" content-name="bilinear" content-type="related">
    For explicit applications of bilinear transformations,
    see the <a href="#bibliographic-notes">Bibliographic Notes</a>.
  </p>

  <hr/>

  <h2>Properties of the learned task representation</h2>
  <p>
    As hinted, in adopting the FiLM perspective we implicitly
    introduce a notion of <em>task representation</em>: each task &mdash; be it
    a question about an image or a painting style to imitate &mdash; elicits a
    different set of FiLM parameters via the FiLM generator which can be
    understood as its representation in terms of how to modulate the FiLMed
    network. To help better understand the properties of this representation,
    let's focus on two FiLMed models used in fairly different problem settings:
  </p>
  <ul>
    <li>
      The visual reasoning model of Perez et al.
      <d-cite key="perez2017learning,perez2018film"></d-cite>, which uses FiLM
      to modulate a visual processing pipeline based off an input question.
      <figure class="l-body-outset">
        <%= require("../static/diagrams/film-clevr.svg") %>
      </figure>
    </li>
    <li>
      The artistic style transfer model of Ghiasi et al.
      <d-cite key="ghiasi2017exploring"></d-cite>, which uses FiLM to modulate a
      feed-forward style transfer network based off an input style image.
      <figure class="l-body-outset">
        <%= require("../static/diagrams/alrfas.svg") %>
      </figure>
    </li>
  </ul>
  <p>
    As a starting point, can we discern any pattern in the FiLM parameters as a
    function of the task description? It could be that FiLM parameters follow a
    simple distribution, indicating that perhaps FiLM is used in consistent ways
    throughout the network. It could also be that there is more structure to the
    FiLM parameter distribution, perhaps because FiLM serves multiple purposes
    in the network.
  </p>
  <p>
    One way to visualize the FiLM parameter space is to plot
    <d-math>\gamma</d-math> against <d-math>\beta</d-math>, with each point
    corresponding to a specific task description and a specific feature map.  If
    we color-code each point according to the feature map it belongs to we
    observe the following:
  </p>
  <figure class="l-body-outset" id="gamma-beta-diagram">
    <%= require("../static/diagrams/gamma-beta.svg") %>
  </figure>
  <p>
    The plots above allow us to make several interesting observations.  First,
    FiLM parameters cluster by feature map in parameter space, and the cluster
    locations are not uniform across feature maps. The orientation of these
    clusters is also not uniform across feature maps: the main axis of variation
    can be <d-math>\gamma</d-math>-aligned, <d-math>\beta</d-math>-aligned, or
    diagonal at varying angles. These findings suggest that the affine
    transformation in FiLM layers is not modulated in a single, consistent way,
    i.e., mostly using <d-math>\gamma</d-math> only, <d-math>\beta</d-math>
    only, or <d-math>\gamma</d-math> and <d-math>\beta</d-math> together in some
    specific way. Maybe this is due to the affine transformation being
    overspecified, or maybe this shows that FiLM layers can be used to perform
    modulation operations in several distinct ways.
  </p>
  <p>
    Nevertheless, the fact that these parameter clusters are often somewhat
    "dense" may help explain why the style transfer model of Ghiasi et al.
    <d-cite key="ghiasi2017exploring"></d-cite> is able to perform style
    interpolations: any convex combination of FiLM parameters is likely to
    correspond to a meaningful parametrization of the FiLMed network.
  </p>
  <figure class="l-body" id="style-interpolation-diagram">
    <%= require("../static/diagrams/style-interpolation.svg") %>
  </figure>
  <p>
    To some extent, the notion of interpolating between tasks using FiLM
    parameters can be applied even in the visual question-answering setting.
    Using the model trained in Perez et al. <d-cite key="perez2018film"></d-cite>,
    we interpolated between the model's FiLM parameters for two pairs of CLEVR
    questions. Here we visualize the input locations responsible for
    the globally max-pooled features fed to the visual pipeline's output classifier:
  </p>
  <figure class="l-body" id="question-interpolation-diagram">
    <%= require("../static/diagrams/question-interpolation.svg") %>
  </figure>
  <p>
    One interesting difference across the two problem settings is that, for the
    visual reasoning model, FiLM parameters sometimes form several sub-clusters
    for a given feature map, which is not true for the style transfer model.
  </p>
  <figure class="l-body-outset" id="clevr-subcluster-diagram">
    <%= require("../static/diagrams/clevr-subcluster.svg") %>
  </figure>
  <p>
    At the very least, this may indicate that FiLM learns to operate in ways
    that are problem-specific, and that we should not expect to find a unified
    and problem-independent explanation for FiLM's success in modulating FiLMed
    networks. Perhaps the compositional or discrete nature of visual reasoning
    requires the model to implement several well-defined modes of operation
    which are less necessary for style transfer.
  </p>
  <p>
    The presence of sub-clusters in the case of the visual reasoning model also
    suggests that question interpolations may not always work reliably, but
    these sub-clusters don't preclude one from performing arithmetic on the
    question representations, as Perez et al. <d-cite key="perez2018film"></d-cite>
    reported.
  </p>
  <figure class="l-body">
    <%= require("../static/diagrams/analogy.svg") %>
  </figure>
  <p>
    Focusing on individual feature maps which exhibit sub-clusters, we can try
    to infer how questions regroup by color-coding the scatter plots by question
    type.
  </p>
  <figure class="l-body-outset" id="clevr-subcluster-color-diagram">
    <%= require("../static/diagrams/clevr-subcluster-color.svg") %>
  </figure>
  <p>
    Sometimes a clear pattern emerges, as in the right plot, where questions
    related to counting and integer equality concentrate in the top-right
    cluster. Sometimes it is harder to draw a conclusion, as in the left plot,
    where question types are scattered across the three clusters.
  </p>
  <p>
    In cases where question types alone cannot explain the clustering of the
    FiLM parameters, we can turn to the conditioning content itself to gain
    an understanding of the mechanism at play. Let's take a look at the same
    feature map scatter plots, but this time see if coloring gamma-beta parameters
    by the words in the questions can explain these cluster shapes.
  </p>
  <figure class="l-body-outset" id="clevr-subcluster-color-words-diagram">
    <%= require("../static/diagrams/clevr-subcluster-color-words.svg") %>
  </figure>
  <p>
    In the left plot (feature map 26), the left subcluster corresponds
    to questions discussing objects positioned "in front" of other objects, while
    the right subcluster to those "behind" other objects.
    In the right plot we see some evidence of separation based
    on object materials. The left subcluster corresponds to questions involving matte
    and rubber objects, while the right subcluster contains questions about shiny, metal,
    or metallic objects.
  </p>
  <p>
    The existence of this structure has already been explored, albeit more
    indirectly, by Ghiasi et al. <d-cite key="ghiasi2017exploring"></d-cite>
    as well as Perez et al. <d-cite key="perez2018film"></d-cite>, who
    applied t-SNE <d-cite key="maaten2008visualizing"></d-cite> on the FiLM
    parameter values.
  </p>
  <figure class="l-body-outset" id="tsne-diagram">
    <%= require("../static/diagrams/tsne.svg") %>
  </figure>
  <p>
    The projection on the left is inspired by a similar projection done by Perez
    et al. <d-cite key="perez2018film"></d-cite> for their visual reasoning
    model trained on CLEVR and shows how questions group by question type.
  </p>
  <p>
    The projection on the right is inspired by a similar projection done by
    Ghiasi et al. <d-cite key="ghiasi2017exploring"></d-cite> for their style
    transfer network. The projection does not cluster artists as neatly as the
    projection on the left, but this is to be expected, given that an artist's
    style may vary widely over time. However, we can still detect interesting
    patterns in the projection: note for instance the isolated cluster (circled
    in the figure) in which paintings by Ivan Shishkin and Rembrandt are
    aggregated. While these two painters exhibit fairly different styles, the
    cluster is a grouping of their sketches.
  </p>
  <figure class="l-body" id="style-explained-diagram">
    <%= require("../static/diagrams/style-explained.svg") %>
  </figure>
  <p>
    To summarize, the way neural networks learn to use FiLM layers seems to
    vary problem to problem, input to input, and even feature to
    feature; there does not seem to be a single mechanism by which the
    network uses FiLM to condition computation. This flexibility may
    explain why FiLM-related methods have been successful across such a
    wide variety of domains.
  </p>
  <p>
    On the other hand, the general applicability of feature-wise transformations
    may make them less suited to tasks requiring a strong inductive bias to get
    good generalization properties. For instance, in the domain of visual
    reasoning, Compositional Attention Networks
    <d-cite key="arad2018compositional"></d-cite> outperform
    FiLM on the CLEVR task by explicitly biasing the model towards
    learning multiple, compositional reasoning steps. One can also
    combine FiLM with other inductive biases or architectural priors,
    such as weight decay and attention mechanisms, though the nature
    of FiLM's interaction with them is currently an open question. In particular,
    it is unclear to what extent the architecture of the FiLM generator influences
    the emergence of linear task representations &mdash; the observed property of
    FiLM architectures that allow for task interpolations, analogies, and question
    arithmetic on the CLEVR task &mdash; as well as its generalization performance.
    Does it arise from inserting FiLM layers into a convolutional network or is
    it due to the use of particular building blocks, such as GRU and ReLU,
    that enforce linearity? We find such questions intriguing, and we
    hope that future work will shine some light on this.
  </p>

  <hr/>

  <h2>Conclusion</h2>
  <p>
    Precisely how changes on the feature level
    alone are able to compound into large and meaningful modulations of the
    FiLMed network is still an open question, and hopefully we will gain a
    better idea in the future.
    For now, though, it is a question evokes the even grander mystery of
    how neural networks in general compound simple operations like
    matrix multiplications and element-wise non-linearities into
    semantically meaningful transformations.
  </p>
</d-article>

<d-appendix>
  <h3 id="bibliographic-notes">Bibliographic Notes</h3>
  <p>
      Multiplicative interactions have succeeded on various tasks, ever since
      they were introduced in vision as "mapping units" <d-cite key="hinton1981a"></d-cite>
      and "dynamic mappings" <d-cite key="vonderMalsburg1994the"></d-cite>
      around 40 years ago.  These tasks include Character-level Language
      Modeling<d-cite key="sutskever2011generating"></d-cite>,
      Image Denoising<d-cite key="tang2012boltzmann"></d-cite>,
      Pose Estimation<d-cite key="taylor2009factored"></d-cite>,
      Tracking<d-cite key="ross2006combining,denil2012learning"></d-cite>,
      Action Recognition<d-cite key="le2011learning,taylor2010convolutional"></d-cite>,
      and, more generally, tasks involving relating or matching inputs, such as
      from different modalities or points in time
      <d-cite key="memisevic2013learning"></d-cite>.
  </p>
  <p>
    Many models lie on the spectrum between FiLM and Hypernetworks:
  </p>
  <ul>
    <li>
      Adaptive CNN <d-cite key="kang2017incorporating"></d-cite> predicts the
      value of several of the model's convolution filters as a function of
      auxiliary inputs like camera perspective, level of noise, etc. The
      resulting convolution filters turn out to be very effective in difficult
      vision tasks such as crowd counting or image deblurring.
    </li>
    <li>
      Residual Adapters <d-cite key="rebuffi2017residualadapters"></d-cite> also
      propose to predict entire convolutional filters conditioned on the visual
      recognition domain they are operating in.
    </li>
    <li>
      In zero-shot/one-shot learning, Ba et al.
      <d-cite key="lei2015predicting"></d-cite> propose a model that predicts
      convolutional filters and classifiers weights based on textual
      descriptions of object classes.
    </li>
    <li>
      In reinforcement learning, Oh et al. <d-cite key="oh2017zero"></d-cite>
      propose a model that computes the parameters of a
      convolutional policy network conditioned on the task description.
    </li>
  </ul>
  <p>
    Tenenbaum and Freeman <d-cite key="tenenbaum1997separating"></d-cite> first
    introduced bilinear models in the vision community to better disentangle
    latent perceptual factors. The authors wanted to separate an image's style
    from its content, arguing that classic linear models were not rich enough to
    extract such complex interaction. They demonstrate the effectiveness of
    their approach by applying it to spoken vowel identification or zero-shot
    font classification. Notable applications include:
  </p>
  <ul>
    <li>
      Chuang et al. <d-cite key="chuang2002facial"></d-cite> perform facial
      animation using bilinear transformations by separating key facial features
      (the style) from visual emotions (the content). Their method can modify
      a speaking subject's expression in recorded sequence from happy to angry
      or neutral.
    </li>
    <li>
      Chu and Park <d-cite key="chu2009personalized"></d-cite> and Yang et al.
      <d-cite key="yang2011like"></d-cite> apply bilinear models to
      recommendation systems by extracting user and item information in various
      settings. More generally, recommendation systems rely heavily on matrix
      factorization methods <d-cite key="koren2009matrix"></d-cite>, which can
      be viewed as a bilinear model where one of the latent vectors is
      fixed<d-cite key="tenenbaum1997separating"></d-cite>.
    </li>
    <li>
      More recently, bilinear models have inspired new neural architectures in
      visual recognition <d-cite key="lin2015bilinear"></d-cite>, video action
      recognition <d-cite key="feichtenhofer2016convolutional"></d-cite>, and
      visual question-answering<d-cite key="fukui2016multimodal"></d-cite>.
    </li>
  </ul>
  <h3>Acknowledgements</h3>
  <p>
    This article would be nowhere near where it is today without the honest and
    constructive feedback we received from various people across several
    organizations. We would like to thank Chris Olah from the Distill editorial
    team for being so generous with his time and advice. We would also like to
    thank Archy de Berker, Xavier Snelgrove, Pedro Oliveira Pinheiro, Alexei
    Nordell-Markovits, Masha Krol, and Minh Dao from Element AI; Roland
    Memisevic from TwentyBN; Dzmitry Bahdanau from MILA; Ameesh Shah from Rice;
    Olivier Pietquin and Jon Shlens from Goolgle Brain; and Jrmie Mary from
    Criteo.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
